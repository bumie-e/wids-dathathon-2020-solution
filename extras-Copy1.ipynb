{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('training_v2.csv')\n",
    "test = pd.read_csv('unlabeled.csv')\n",
    "wid_dict = pd.read_csv('WiDS Datathon 2020 Dictionary.csv')\n",
    "sol_temp = pd.read_csv('solution_template.csv')\n",
    "sam = pd.read_csv('samplesubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "#pd.set_option('display.max_width', 1000)\n",
    "wid_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.icu_stay_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train.columns:\n",
    "    print(train[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.isnull().sum()/len(train)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na = train.isnull().sum()/len(train)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=all_data_na.hospital_death\n",
    "all_data_na.drop('hospital_death', inplace=True)\n",
    "\n",
    "all_data_na.drop(all_data_na[all_data_na > 10].index, inplace=True) # drop rows with percents greater than 10\n",
    "percent_null = pd.DataFrame({'null_cols': all_data_na, 'dataTypes': train[all_data_na.index].dtypes})\n",
    "#percent_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "cat_var = encoder.fit_transform(train['Accident & Emergency'])\n",
    "cat_var\n",
    "\n",
    "#to get the order of arrangement,\n",
    "print(encoder.classes_)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "Oencoder = OneHotEncoder()\n",
    "one_var = Oencoder.fit_transform(cat_var.reshape(-1, 1))\n",
    "one_var\n",
    "\n",
    "\n",
    "'''We can apply both transformations (from text categories to integer categories, then\n",
    "from integer categories to one-hot vectors) in one shot using the LabelBinarizer\n",
    "class:'''\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "cat_var_hot = lb.fit_transform(train['Accident & Emergency'])\n",
    "cat_var_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Custom Transformers\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', Imputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''how can you join these transformations into a single\n",
    "pipeline? Scikit-Learn provides a FeatureUnion class for this. You give it a list of\n",
    "transformers (which can be entire transformer pipelines), and when its transform()\n",
    "method is called it runs each transformer’s transform() method in parallel, waits for\n",
    "their output, and then concatenates them and returns the result (and of course calling\n",
    "its fit() method calls all each transformer’s fit() method). A full pipeline handling\n",
    "both numerical and categorical attributes may look like this:'''\n",
    "\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(num_attribs)),\n",
    "    ('imputer', Imputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(cat_attribs)),\n",
    "    ('label_binarizer', LabelBinarizer()),\n",
    "])\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline),\n",
    "])\n",
    "\n",
    "And you can run the whole pipeline simply:\n",
    ">>> housing_prepared = full_pipeline.fit_transform(housing)\n",
    ">>> housing_prepared\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "Let’s look at the results:\n",
    "    \n",
    ">>> def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    ">>> display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to save your model instead of using pickle\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(my_model, \"my_model.pkl\")\n",
    "# and later...\n",
    "my_model_loaded = joblib.load(\"my_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "When you have no idea what value a hyperparameter should have,\n",
    "a simple approach is to try out consecutive powers of 10 (or a\n",
    "smaller number if you want a more fine-grained search, as shown\n",
    "in this example with the n_estimators hyperparameter).\n",
    "\n",
    "grid_search.best_params_\n",
    "# gives the best max_features and n_estimators\n",
    "\n",
    "You can also get the best estimator directly:\n",
    ">>> grid_search.best_estimator_\n",
    "\n",
    "And of course the evaluation scores are also available:\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n",
    "    \n",
    "    \n",
    "The grid search approach is fine when you are exploring relatively few combinations,\n",
    "like in the previous example, but when the hyperparameter search space is large, it is\n",
    "often preferable to use RandomizedSearchCV instead. This class can be used in much\n",
    "the same way as the GridSearchCV class, but instead of trying out all possible combinations,\n",
    "it evaluates a given number of random combinations by selecting a random\n",
    "value for each hyperparameter at every iteration. This approach has two main benefits:\n",
    "• If you let the randomized search run for, say, 1,000 iterations, this approach will\n",
    "explore 1,000 different values for each hyperparameter (instead of just a few values\n",
    "per hyperparameter with the grid search approach).\n",
    "• You have more control over the computing budget you want to allocate to hyperparameter\n",
    "search, simply by setting the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = train[percent_null.index]\n",
    "\n",
    "test1 =test[percent_null.index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_id = train['encounter_id']\n",
    "train1.drop(['patient_id', 'hospital_id', 'encounter_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# A function to plot a boxplot for each column in train1\n",
    "for k in train1.keys():\n",
    "    plt.boxplot(train1[k])\n",
    "    plt.title('boxplot for column ' + k)\n",
    "    plt.grid('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train = train[['icu_type', 'icu_stay_type', 'icu_admit_source', 'gender', 'apache_2_bodysystem', 'ethnicity', 'apache_3j_bodysystem',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train1.columns:\n",
    "    if i not in cat_train.columns:\n",
    "        train1[i].fillna(train[i].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(x, y):\n",
    "    fig, ax = plt.subplots(figsize = (15, 5))\n",
    "    sns.barplot(x, y)\n",
    "    plt.xlabel('x', fontSize=13)\n",
    "    plt.xticks(rotation='90')\n",
    "    plt.ylabel('Hospital_death', fontSize=13)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train.icu_admit_source.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(cat_train.icu_admit_source, train.hospital_death, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.icu_admit_source = train1.icu_admit_source.fillna('Accident & Emergency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.icu_admit_source = test1.icu_admit_source.fillna('Accident & Emergency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train.gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.gender = train1.gender.fillna('Not_Sure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train.apache_2_bodysystem.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.apache_2_bodysystem.fillna('Undefined Diagnoses', inplace=True)\n",
    "train1.apache_2_bodysystem.replace('Undefined diagnoses', 'Undefined Diagnoses', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train.ethnicity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(cat_train.ethnicity, train.hospital_death, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.ethnicity.fillna('Other/Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train.apache_3j_bodysystem.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.apache_3j_bodysystem = train1.apache_3j_bodysystem.fillna('Undefined_Diagnoses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "for i in cat_train.columns:\n",
    "    plot_graph(train1[i], train['hospital_death'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1['icu_type'][train1['icu_type'] == 'MICU'] = 0\n",
    "train1['icu_type'][train1['icu_type'] == 'Cardiac ICU'] = 1\n",
    "train1['icu_type'][train1['icu_type'] == 'Med-Surg ICU'] = 2\n",
    "train1['icu_type'][train1['icu_type'] == 'Neuro ICU'] = 3\n",
    "train1['icu_type'][train1['icu_type'] == 'CCU-CTICU'] = 4\n",
    "train1['icu_type'][train1['icu_type'] == 'SICU'] = 4\n",
    "train1['icu_type'][train1['icu_type'] == 'CTICU'] = 5\n",
    "train1['icu_type'][train1['icu_type'] == 'CSICU'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1['icu_stay_type'][train1['icu_stay_type'] == 'transfer'] = 0\n",
    "train1['icu_stay_type'][train1['icu_stay_type'] == 'readmit'] = 1\n",
    "train1['icu_stay_type'][train1['icu_stay_type'] == 'admit'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1['icu_admit_source'][train1['icu_admit_source'] == 'Other ICU'] = 0\n",
    "train1['icu_admit_source'][train1['icu_admit_source'] == 'Other Hospital'] = 1\n",
    "train1['icu_admit_source'][train1['icu_admit_source'] == 'Floor'] = 2\n",
    "train1['icu_admit_source'][train1['icu_admit_source'] == 'Accident & Emergency'] = 3\n",
    "train1['icu_admit_source'][train1['icu_admit_source'] == 'Operating Room / Recovery'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1['apache_2_bodysystem'][train1['apache_2_bodysystem'] == 'Respiratory'] = 0\n",
    "train1['apache_2_bodysystem'][train1['apache_2_bodysystem'] == 'Cardiovascular'] = 1\n",
    "train1['apache_2_bodysystem'][train1['apache_2_bodysystem'] == 'Haematologic'] = 2\n",
    "train1['apache_2_bodysystem'][train1['apache_2_bodysystem'] == 'Undefined Diagnoses'] = 7\n",
    "train1['apache_2_bodysystem'][train1['apache_2_bodysystem'] == 'Undefined diagnoses'] = 7\n",
    "train1['apache_2_bodysystem'][train1['apache_2_bodysystem'] == 'Neurologic'] = 3\n",
    "train1['apache_2_bodysystem'][train1['apache_2_bodysystem'] == 'Trauma'] = 5\n",
    "train1['apache_2_bodysystem'][train1['apache_2_bodysystem'] == 'Gastrointestinal'] = 4\n",
    "train1['apache_2_bodysystem'][train1['apache_2_bodysystem'] == 'Renal/Genitourinary'] = 6\n",
    "train1['apache_2_bodysystem'][train1['apache_2_bodysystem'] == 'Metabolic'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1['apache_3j_bodysystem'][train1['apache_3j_bodysystem'] == 'Sepsis'] = 0\n",
    "train1['apache_3j_bodysystem'][train1['apache_3j_bodysystem'] == 'Respiratory'] = 1\n",
    "train1['apache_3j_bodysystem'][train1['apache_3j_bodysystem'] == 'Hematological'] = 2\n",
    "train1['apache_3j_bodysystem'][train1['apache_3j_bodysystem'] == 'Undefined_Diagnoses'] = 9\n",
    "train1['apache_3j_bodysystem'][train1['apache_3j_bodysystem'] == 'Neurological'] = 4\n",
    "train1['apache_3j_bodysystem'][train1['apache_3j_bodysystem'] == 'Cardiovascular'] = 3\n",
    "train1['apache_3j_bodysystem'][train1['apache_3j_bodysystem'] == 'Gastrointestinal'] = 5\n",
    "train1['apache_3j_bodysystem'][train1['apache_3j_bodysystem'] == 'Trauma'] = 6\n",
    "train1['apache_3j_bodysystem'][train1['apache_3j_bodysystem'] == 'Genitourinary'] = 7\n",
    "train1['apache_3j_bodysystem'][train1['apache_3j_bodysystem'] == 'Musculoskeletal/Skin'] = 8\n",
    "train1['apache_3j_bodysystem'][train1['apache_3j_bodysystem'] == 'Metabolic'] = 10\n",
    "train1['apache_3j_bodysystem'][train1['apache_3j_bodysystem'] == 'Gynecological'] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in test1.columns:\n",
    "    if i not in cat_train.columns:\n",
    "        test1[i].fillna(test1[i].median(), inplace = True)\n",
    "        \n",
    "test1.gender = test1.gender.fillna('Not_Sure')\n",
    "\n",
    "test1.apache_2_bodysystem.fillna('Undefined Diagnoses', inplace=True)\n",
    "test1.apache_2_bodysystem.replace('Undefined diagnoses', 'Undefined Diagnoses', inplace=True)\n",
    "\n",
    "test1.ethnicity.fillna('Other/Unknown', inplace=True)\n",
    "\n",
    "test1.apache_3j_bodysystem = test1.apache_3j_bodysystem.fillna('Undefined_Diagnoses')\n",
    "\n",
    "test1['icu_type'][test1['icu_type'] == 'MICU'] = 0\n",
    "test1['icu_type'][test1['icu_type'] == 'Cardiac ICU'] = 1\n",
    "test1['icu_type'][test1['icu_type'] == 'Med-Surg ICU'] = 2\n",
    "test1['icu_type'][test1['icu_type'] == 'Neuro ICU'] = 3\n",
    "test1['icu_type'][test1['icu_type'] == 'CCU-CTICU'] = 4\n",
    "test1['icu_type'][test1['icu_type'] == 'SICU'] = 5\n",
    "test1['icu_type'][test1['icu_type'] == 'CTICU'] = 6\n",
    "test1['icu_type'][test1['icu_type'] == 'CSICU'] = 7\n",
    "\n",
    "test1['icu_stay_type'][test1['icu_stay_type'] == 'transfer'] = 0\n",
    "test1['icu_stay_type'][test1['icu_stay_type'] == 'readmit'] = 1\n",
    "test1['icu_stay_type'][test1['icu_stay_type'] == 'admit'] = 2\n",
    "\n",
    "\n",
    "test1['apache_2_bodysystem'][test1['apache_2_bodysystem'] == 'Respiratory'] = 0\n",
    "test1['apache_2_bodysystem'][test1['apache_2_bodysystem'] == 'Cardiovascular'] = 1\n",
    "test1['apache_2_bodysystem'][test1['apache_2_bodysystem'] == 'Haematologic'] = 2\n",
    "test1['apache_2_bodysystem'][test1['apache_2_bodysystem'] == 'Undefined Diagnoses'] = 8\n",
    "test1['apache_2_bodysystem'][test1['apache_2_bodysystem'] == 'Undefined diagnoses'] = 8\n",
    "test1['apache_2_bodysystem'][test1['apache_2_bodysystem'] == 'Neurologic'] = 3\n",
    "test1['apache_2_bodysystem'][test1['apache_2_bodysystem'] == 'Trauma'] = 5\n",
    "test1['apache_2_bodysystem'][test1['apache_2_bodysystem'] == 'Gastrointestinal'] = 4\n",
    "test1['apache_2_bodysystem'][test1['apache_2_bodysystem'] == 'Renal/Genitourinary'] = 6\n",
    "test1['apache_2_bodysystem'][test1['apache_2_bodysystem'] == 'Metabolic'] = 7\n",
    "\n",
    "test1['apache_3j_bodysystem'][test1['apache_3j_bodysystem'] == 'Sepsis'] = 0\n",
    "test1['apache_3j_bodysystem'][test1['apache_3j_bodysystem'] == 'Respiratory'] = 1\n",
    "test1['apache_3j_bodysystem'][test1['apache_3j_bodysystem'] == 'Hematological'] = 2\n",
    "test1['apache_3j_bodysystem'][test1['apache_3j_bodysystem'] == 'Undefined_Diagnoses'] = 10\n",
    "test1['apache_3j_bodysystem'][test1['apache_3j_bodysystem'] == 'Neurological'] = 4\n",
    "test1['apache_3j_bodysystem'][test1['apache_3j_bodysystem'] == 'Cardiovascular'] = 3\n",
    "test1['apache_3j_bodysystem'][test1['apache_3j_bodysystem'] == 'Gastrointestinal'] = 5\n",
    "test1['apache_3j_bodysystem'][test1['apache_3j_bodysystem'] == 'Trauma'] = 6\n",
    "test1['apache_3j_bodysystem'][test1['apache_3j_bodysystem'] == 'Genitourinary'] = 7\n",
    "test1['apache_3j_bodysystem'][test1['apache_3j_bodysystem'] == 'Musculoskeletal/Skin'] = 8\n",
    "test1['apache_3j_bodysystem'][test1['apache_3j_bodysystem'] == 'Metabolic'] = 9\n",
    "test1['apache_3j_bodysystem'][test1['apache_3j_bodysystem'] == 'Gynecological'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1['icu_admit_source'][test1['icu_admit_source'] == 'Other ICU'] = 0\n",
    "test1['icu_admit_source'][test1['icu_admit_source'] == 'Other Hospital'] = 1\n",
    "test1['icu_admit_source'][test1['icu_admit_source'] == 'Floor'] = 2\n",
    "test1['icu_admit_source'][test1['icu_admit_source'] == 'Accident & Emergency'] = 3\n",
    "test1['icu_admit_source'][test1['icu_admit_source'] == 'Operating Room / Recovery'] = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.get_dummies(train1, columns=['ethnicity', 'gender'])\n",
    "test1 = pd.get_dummies(test1, columns=['ethnicity', 'gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.drop(['patient_id', 'hospital_id', 'encounter_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train = train[['icu_type', 'icu_stay_type', 'icu_admit_source', 'apache_2_bodysystem','apache_3j_bodysystem']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.hospital_death.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.icu_type = pd.to_numeric(train1.icu_type)\n",
    "train1.icu_stay_type = pd.to_numeric(train1.icu_stay_type)\n",
    "train1.icu_admit_source = pd.to_numeric(train1.icu_admit_source)\n",
    "train1.apache_2_bodysystem = pd.to_numeric(train1.apache_2_bodysystem)\n",
    "train1.apache_3j_bodysystem = pd.to_numeric(train1.apache_3j_bodysystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.icu_type = pd.to_numeric(test1.icu_type)\n",
    "test1.icu_stay_type = pd.to_numeric(test1.icu_stay_type)\n",
    "test1.icu_admit_source = pd.to_numeric(test1.icu_admit_source)\n",
    "test1.apache_2_bodysystem = pd.to_numeric(test1.apache_2_bodysystem)\n",
    "test1.apache_3j_bodysystem = pd.to_numeric(test1.apache_3j_bodysystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train.hospital_death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.shape, test1.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "nrm = Normalizer()\n",
    "std = StandardScaler()\n",
    "minmax = MinMaxScaler()\n",
    "n_train = nrm.fit_transform(train1)\n",
    "s_train = std.fit_transform(n_train)\n",
    "m_train = minmax.fit_transform(s_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(m_train, target, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=40, whiten=True).fit(x_train)\n",
    "train_pca = pca.transform(x_train)\n",
    "test_pca = pca.transform(x_test)\n",
    "print(train_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3, whiten=True).fit(itest)\n",
    "itest_pca = pca.transform(itest)\n",
    "print(itest_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda =LDA(n_components=35)\n",
    "X_train = lda.fit_transform(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = lda.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = lda.transform(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression(normalize=True)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = lr.predict(X_test)\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"RMSE:\", sqrt(mean_squared_error(y_test, p)))\n",
    "#f1_score(p, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression()\n",
    "LR.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LR.predict(x_test)\n",
    "p = LR.predict_proba(x_test)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#p = LR.predict_proba(X_test)[]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(a, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = LR.predict_proba(t1)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data = {'encounter_id':test['encounter_id'], 'hospital_death':m1})\n",
    "output.to_csv(path_or_buf = '031.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(base_score=0.3, learning_rate = 0.02,\n",
    "                    num_boosting_rounds = 270,\n",
    "                    n_estimators= 600, \n",
    "                    gamma=0.1,max_depth=10, \n",
    "                    subsample=0.8)\n",
    "xgb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = xgb.predict(x_test)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(a, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mm:\n",
    "    if i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = xgb.predict_proba(t1)[:, 1]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l = xgb.predict_proba(t1)[1\n",
    "output = pd.DataFrame(data = {'encounter_id':test['encounter_id'], 'hospital_death':l})\n",
    "output.to_csv(path_or_buf = 'b2.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "gr = GradientBoostingClassifier(learning_rate=0.05, n_estimators=500, random_state=42)\n",
    "gr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = gr.predict(X_test)\n",
    "roc_auc_score(y_test, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE:\", sqrt(mean_squared_error(y_test, p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = gr.predict(test1)\n",
    "output = pd.DataFrame(data = {'encounter_id':test['encounter_id'], 'hospital_death':pre})\n",
    "output.to_csv(path_or_buf = 'a2.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=1000)\n",
    "model.fit(X_train, y_train, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=model.predict(X_test)\n",
    "#print(\"RMSE:\", sqrt(mean_squared_error(y_test, p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict_proba(test1)[:, 0]\n",
    "output = pd.DataFrame(data = {'encounter_id':test['encounter_id'], 'hospital_death':p})\n",
    "output.to_csv(path_or_buf = '04.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(0, 86)\n",
    "cat_features = list(a)\n",
    "print(cat_features)\n",
    "\n",
    "\n",
    "from catboost import Pool\n",
    "pool = Pool(data=train1, label=target)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = train_test_split(train1, target, test_size=0.2, random_state=0, stratify=target)\n",
    "X_train, X_validation, y_train, y_validation = data\n",
    "\n",
    "train_pool = Pool(\n",
    "    data=X_train, \n",
    "    label=y_train\n",
    ")\n",
    "\n",
    "validation_pool = Pool(\n",
    "    data=X_validation, \n",
    "    label=y_validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.03,\n",
    "    custom_loss=['AUC', 'Accuracy']\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_pool,\n",
    "    eval_set=validation_pool,\n",
    "    verbose=False,\n",
    "    plot=True\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict_proba(test1)[:, 0]\n",
    "output = pd.DataFrame(data = {'encounter_id':test['encounter_id'], 'hospital_death':p})\n",
    "output.to_csv(path_or_buf = '05.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "model = CatBoostRegressor(\n",
    "    iterations=2500,\n",
    "    learning_rate=0.015,\n",
    "    loss_function='RMSE'\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_pool,\n",
    "    eval_set=validation_pool,\n",
    "    verbose=False,\n",
    "    plot=True\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(test1)\n",
    "output = pd.DataFrame(data = {'encounter_id':test['encounter_id'], 'hospital_death':p})\n",
    "output.to_csv(path_or_buf = '07.csv', index = False, quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import cv\n",
    "\n",
    "params = {\n",
    "    'loss_function': 'RMSE',\n",
    "    'iterations': 2000,\n",
    "    'custom_loss': 'AUC',\n",
    "    'learning_rate': 0.02,\n",
    "}\n",
    "\n",
    "cv_data = cv(\n",
    "    params = params,\n",
    "    pool = train_pool,\n",
    "    fold_count=10,\n",
    "    shuffle=True,\n",
    "    partition_random_seed=0,\n",
    "    plot=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_value = np.min(cv_data['test-Logloss-mean'])\n",
    "best_iter = np.argmin(cv_data['test-Logloss-mean'])\n",
    "\n",
    "print('Best validation Logloss score, not stratified: {:.4f}±{:.4f} on step {}'.format(\n",
    "    best_value,\n",
    "    cv_data['test-Logloss-std'][best_iter],\n",
    "    best_iter)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
